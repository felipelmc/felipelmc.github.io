[
  {
    "objectID": "publications/publications.html",
    "href": "publications/publications.html",
    "title": "üóÉPublications",
    "section": "",
    "text": "üìÑ As rela√ß√µes Executivo-Legislativo na Primeira Rep√∫blica: uma an√°lise das mensagens presidenciais ao Congresso (1910-1920). Mosaico. 2021. [pdf]\n\n\n\nüìÑ Fatores explicativos do apoio ou rejei√ß√£o √† cria√ß√£o de uma pol√≠tica municipal de renda b√°sica: modelando a percep√ß√£o dos cariocas. 2025.\nJoint work with Jimmy Medeiros (FGV CPDOC), Ant√¥nio Mariano (FGV CPDOC), and Philippe Chaves Guedon (FGV CPDOC)."
  },
  {
    "objectID": "publications/publications.html#papers",
    "href": "publications/publications.html#papers",
    "title": "üóÉPublications",
    "section": "",
    "text": "üìÑ As rela√ß√µes Executivo-Legislativo na Primeira Rep√∫blica: uma an√°lise das mensagens presidenciais ao Congresso (1910-1920). Mosaico. 2021. [pdf]\n\n\n\nüìÑ Fatores explicativos do apoio ou rejei√ß√£o √† cria√ß√£o de uma pol√≠tica municipal de renda b√°sica: modelando a percep√ß√£o dos cariocas. 2025.\nJoint work with Jimmy Medeiros (FGV CPDOC), Ant√¥nio Mariano (FGV CPDOC), and Philippe Chaves Guedon (FGV CPDOC)."
  },
  {
    "objectID": "projects/scielo-summarizer/index.html",
    "href": "projects/scielo-summarizer/index.html",
    "title": "SciELO-Summarizer",
    "section": "",
    "text": "SciELO-Summarizer consists of a summarizer for scientific articles in Portuguese. It scrapes the content of the article from the SciELO website and generates a personalized summary for the user using Large Language Models (LLMs), especifically Llama3.\nTo run the project locally, you need to have both Python 3.8 or higher and Llama3 installed. It is also possible to run the project on Google Colab, following this link.\nThe program was developed during the Summer Institute in Computational Social Sciences (SICSS) 2024, hosted by FGV ECMI, Brazil, in July 2024."
  },
  {
    "objectID": "projects/scielo-summarizer/index.html#about-the-project",
    "href": "projects/scielo-summarizer/index.html#about-the-project",
    "title": "SciELO-Summarizer",
    "section": "",
    "text": "SciELO-Summarizer consists of a summarizer for scientific articles in Portuguese. It scrapes the content of the article from the SciELO website and generates a personalized summary for the user using Large Language Models (LLMs), especifically Llama3.\nTo run the project locally, you need to have both Python 3.8 or higher and Llama3 installed. It is also possible to run the project on Google Colab, following this link.\nThe program was developed during the Summer Institute in Computational Social Sciences (SICSS) 2024, hosted by FGV ECMI, Brazil, in July 2024."
  },
  {
    "objectID": "projects/machine-learning/index.html",
    "href": "projects/machine-learning/index.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Nearest Neighbours Method (\\(k\\)-NN) (10/10)\nLinear Regression (8.5/10)\nLogistic Regression and Approximate Bayesian Inference (10/10)\nSelection of Models and Hyperparameters (9.25/10)\nGaussian Processes for Regression (10/10)\nNeural Networks (10/10)\nDimensionality Reduction (10/10)\nK-means and Mixture models (8.5/10)"
  },
  {
    "objectID": "projects/machine-learning/index.html#assignments",
    "href": "projects/machine-learning/index.html#assignments",
    "title": "Machine Learning",
    "section": "",
    "text": "Nearest Neighbours Method (\\(k\\)-NN) (10/10)\nLinear Regression (8.5/10)\nLogistic Regression and Approximate Bayesian Inference (10/10)\nSelection of Models and Hyperparameters (9.25/10)\nGaussian Processes for Regression (10/10)\nNeural Networks (10/10)\nDimensionality Reduction (10/10)\nK-means and Mixture models (8.5/10)"
  },
  {
    "objectID": "projects/machine-learning/index.html#final-project",
    "href": "projects/machine-learning/index.html#final-project",
    "title": "Machine Learning",
    "section": "Final Project",
    "text": "Final Project\nThe final project was co-authored by Ana Carolina Erthal, Guilherme de Melo and Bernardo Vargas. The project implements a way of comparing Conformal Prediction with traditional machine learning approaches to generate confidence intervals."
  },
  {
    "objectID": "projects/datagrid/index.html",
    "href": "projects/datagrid/index.html",
    "title": "DataGrid",
    "section": "",
    "text": "The DataGrid class is specifically designed to work with datasets that follow the structure below:\n\n\n\nColumn\nData Type\nSearch Type\nExtra\n\n\n\n\nid\ninteger\nexact\nunique\n\n\nowner_id\nstring\nexact\nExactly 5 alphanumeric characters\n\n\ncreation_date\nstring\nrange\nFormat: YYYY-MM-DD hh:mm:ss\n\n\ncount\ninteger\nrange\n\n\n\nname\nstring\ncontains\nMaximum length of 20 characters\n\n\ncontent\nstring\ncontains\n\n\n\n\nEach record in the DataGrid is considered an Event.\nTo initialize the DataGrid class, simply import the module and instantiate the class. Make sure your script can access the folder where the DataGrid module is located, for example:\nimport sys\nsys.path.append('src/')\n\nfrom datagrid import DataGrid\nInitialize the DataGrid class with:\ndatagrid = DataGrid()\nThe DataGrid class has the following methods:\n\nread_csv(file, sep = ',', encoding = 'utf-8'): populates the DataGrid from the data in the CSV file whose path is provided as a parameter, considering the specified separator and encoding;\nshow(start=0, end=100, prints = False, returns = True): displays the entries in the DataGrid, limiting the display to the range defined by the parameters. returns=True returns the list of Event objects between start and end, and prints=True shows the content of these objects. It displays the table in its current sorted state.\ninsert_row(row): inserts new events into the DataGrid. It takes a dictionary containing the data of the event to be inserted and creates an Event instance from this data. The dictionary must have the column names as keys and the data to be inserted as values, following the structure described in the table above.\ndelete_row(column, value): removes events from the DataGrid. It takes the name of the column and the value to search for in that column. It removes all events that have the searched value in the specified column. If column = 'positions', it removes elements based on their position (index) in the table. In this case, value can be either a range identified by a tuple (start, end) or a single positive integer.\nsearch(column, value): searches for events in the DataGrid. It takes the name of the column and the value to search for in that column. It returns a list of Event objects that contain the searched value in the specified column.\nsort(column, direction = 'asc'): sorts the DataGrid. It takes the name of the column and the sorting direction. To sort in descending order, simply pass direction = 'desc'.\nselect_count(i, j, how = 'median-of-medians'): returns the list of Event objects between positions i and j in the table, considering the count column sorted in ascending order. This operation does not alter the internal structure of the DataGrid. It is also possible to pass the parameter how = 'quickselect' or how = 'heapsort' to choose which algorithm will be used to perform the operation.\n\nThe file demo.ipynb contains an example of how to use the DataGrid class with data randomly generated by the file dataGenerator.py. The comments on the operations performed in the notebook refer to the results obtained using the file fake_data_100.csv, which contains 100 rows."
  },
  {
    "objectID": "projects/datagrid/index.html#user-guide",
    "href": "projects/datagrid/index.html#user-guide",
    "title": "DataGrid",
    "section": "",
    "text": "The DataGrid class is specifically designed to work with datasets that follow the structure below:\n\n\n\nColumn\nData Type\nSearch Type\nExtra\n\n\n\n\nid\ninteger\nexact\nunique\n\n\nowner_id\nstring\nexact\nExactly 5 alphanumeric characters\n\n\ncreation_date\nstring\nrange\nFormat: YYYY-MM-DD hh:mm:ss\n\n\ncount\ninteger\nrange\n\n\n\nname\nstring\ncontains\nMaximum length of 20 characters\n\n\ncontent\nstring\ncontains\n\n\n\n\nEach record in the DataGrid is considered an Event.\nTo initialize the DataGrid class, simply import the module and instantiate the class. Make sure your script can access the folder where the DataGrid module is located, for example:\nimport sys\nsys.path.append('src/')\n\nfrom datagrid import DataGrid\nInitialize the DataGrid class with:\ndatagrid = DataGrid()\nThe DataGrid class has the following methods:\n\nread_csv(file, sep = ',', encoding = 'utf-8'): populates the DataGrid from the data in the CSV file whose path is provided as a parameter, considering the specified separator and encoding;\nshow(start=0, end=100, prints = False, returns = True): displays the entries in the DataGrid, limiting the display to the range defined by the parameters. returns=True returns the list of Event objects between start and end, and prints=True shows the content of these objects. It displays the table in its current sorted state.\ninsert_row(row): inserts new events into the DataGrid. It takes a dictionary containing the data of the event to be inserted and creates an Event instance from this data. The dictionary must have the column names as keys and the data to be inserted as values, following the structure described in the table above.\ndelete_row(column, value): removes events from the DataGrid. It takes the name of the column and the value to search for in that column. It removes all events that have the searched value in the specified column. If column = 'positions', it removes elements based on their position (index) in the table. In this case, value can be either a range identified by a tuple (start, end) or a single positive integer.\nsearch(column, value): searches for events in the DataGrid. It takes the name of the column and the value to search for in that column. It returns a list of Event objects that contain the searched value in the specified column.\nsort(column, direction = 'asc'): sorts the DataGrid. It takes the name of the column and the sorting direction. To sort in descending order, simply pass direction = 'desc'.\nselect_count(i, j, how = 'median-of-medians'): returns the list of Event objects between positions i and j in the table, considering the count column sorted in ascending order. This operation does not alter the internal structure of the DataGrid. It is also possible to pass the parameter how = 'quickselect' or how = 'heapsort' to choose which algorithm will be used to perform the operation.\n\nThe file demo.ipynb contains an example of how to use the DataGrid class with data randomly generated by the file dataGenerator.py. The comments on the operations performed in the notebook refer to the results obtained using the file fake_data_100.csv, which contains 100 rows."
  },
  {
    "objectID": "projects/datagrid/index.html#random-data-generation",
    "href": "projects/datagrid/index.html#random-data-generation",
    "title": "DataGrid",
    "section": "Random Data Generation",
    "text": "Random Data Generation\nIf you want to generate random data to test the DataGrid module, simply run the file dataGenerator.py. Remember to adjust the value(s) in the n list at the end of the file to define how many files you want to generate and how many rows each should contain."
  },
  {
    "objectID": "news/news.html",
    "href": "news/news.html",
    "title": "üì∞News",
    "section": "",
    "text": "February 27, 2025\nI received the Professor Carlos Eduardo Sarmento Academic Distinction Award from the School of Social Sciences at Funda√ß√£o Getulio Vargas (FGV CPDOC) for achieving the highest academic performance among the 2025 graduates of the CPDOC undergraduate program! üòÉüéì\nDecember 13, 2024\nI have been accepted to pursue a Master‚Äôs degree in Applied Mathematics and Data Science at the School of Applied Mathematics (FGV EMAp). Even though I‚Äôm not accepting the offer, I‚Äôm very grateful for the opportunity and excited to continue my journey in Political Science at IESP-UERJ.\nNovember 26, 2024\nI have successfully delivered my undergraduate thesis, marking the official completion of my dual degrees in Data Science & Artificial Intelligence from the School of Applied Mathematics (FGV EMAp) and Social Sciences from the Superior School of Social Sciences (FGV CPDOC). I‚Äôm very grateful for the support of my family, friends, and advisors throughout this journey, which I‚Äôm very proud of.\nNovember 21, 2024\nI have been accepted in the first position to pursue a Master‚Äôs degree in Political Science at the Institute of Social and Political Studies (IESP-UERJ), starting in March 2025. This opportunity will allow me to contribute to impactful research, particularly in the application of Data Science and AI to the field."
  },
  {
    "objectID": "learning/learning.html",
    "href": "learning/learning.html",
    "title": "Learning",
    "section": "",
    "text": "üìå Techniques and Algorithms in Data Science: an introduction to some of the most important ML algorithms in Data Science ‚Äì regression, neural networks, decision trees, ensemble learning and unsupervised approaches. [repository]\nüìå Numerical Linear Algebra: numerical methods for solving linear algebra problems. [repository]\nüìå Data Visualization: the fundamentals of data visualization using a variety of tools such as D3.js, Python and Power BI. [repository]\nüìå Machine Learning: a more advanced and probabilistic approach to machine learning models, including mixture models and approximate bayesian inference. [repository]\nüìå Deep Learning: the mathematical foundation of neural networks, covering topics such as CNNs, LSTMs, GANs, Transformers, Transfer Learning and deep autoencoders. [repository]\n\n\n\nüìå Lego I: an introduction to quantitative social sciences, covering the fundamentals of data analysis, research design, probability and inference. [repository] [website]\nüìå Pesquisa de Survey: the fundamentals of survey research, including the total survey error paradigm, sampling, questionnaire design, post-stratification and other more advanced topics such as non-response and likely voter models. [repository] [website]\nüìå Teoria Pol√≠tica I: the fundamental problems of political theory, from classical antiquity to the period immediately prior to the revolutions of the 18th century. [repository] [website]\n\n\n\nüìå Studying Math: a series of books I have been reading to keep practicing with math. [repository]"
  },
  {
    "objectID": "learning/learning.html#repositories",
    "href": "learning/learning.html#repositories",
    "title": "Learning",
    "section": "",
    "text": "üìå Techniques and Algorithms in Data Science: an introduction to some of the most important ML algorithms in Data Science ‚Äì regression, neural networks, decision trees, ensemble learning and unsupervised approaches. [repository]\nüìå Numerical Linear Algebra: numerical methods for solving linear algebra problems. [repository]\nüìå Data Visualization: the fundamentals of data visualization using a variety of tools such as D3.js, Python and Power BI. [repository]\nüìå Machine Learning: a more advanced and probabilistic approach to machine learning models, including mixture models and approximate bayesian inference. [repository]\nüìå Deep Learning: the mathematical foundation of neural networks, covering topics such as CNNs, LSTMs, GANs, Transformers, Transfer Learning and deep autoencoders. [repository]\n\n\n\nüìå Lego I: an introduction to quantitative social sciences, covering the fundamentals of data analysis, research design, probability and inference. [repository] [website]\nüìå Pesquisa de Survey: the fundamentals of survey research, including the total survey error paradigm, sampling, questionnaire design, post-stratification and other more advanced topics such as non-response and likely voter models. [repository] [website]\nüìå Teoria Pol√≠tica I: the fundamental problems of political theory, from classical antiquity to the period immediately prior to the revolutions of the 18th century. [repository] [website]\n\n\n\nüìå Studying Math: a series of books I have been reading to keep practicing with math. [repository]"
  },
  {
    "objectID": "learning/learning.html#training-workshops",
    "href": "learning/learning.html#training-workshops",
    "title": "Learning",
    "section": "üí° Training & Workshops",
    "text": "üí° Training & Workshops\n\n[July 2024] Summer Institute in Computational Social Sciences (FGV ECMI)\nCollaborated with international researchers on computational social science projects during a two-week program.\n[July 2024] ML4Good (Centre pour la S√©curit√© de l‚ÄôIA and EffiSciences)\nTen-day bootcamp on AI Safety, focusing on alignment, governance, and responsible AI development."
  },
  {
    "objectID": "blog/rag/index.html",
    "href": "blog/rag/index.html",
    "title": "RAG pipelines: o que s√£o e como constru√≠-los",
    "section": "",
    "text": "Hoje em dia temos acesso uma quantidade razoavelmente extensa de Large Language Models dispon√≠veis para uso, como os oferecidos pela OpenAI, Meta, Anthropic e xAI. Essas redes neurais ‚Äúgrandes‚Äù, normalmente caracterizadas dessa maneira em fun√ß√£o do enorme n√∫mero de par√¢metros estimados durante o processo de treinamento, s√£o frequentemente chamados de foundation models. Tratam-se de modelos treinados em quantidades massivas de dados e que, no fim das contas, servem de base para uma s√©rie de aplica√ß√µes de naturezas distintas, como gera√ß√£o de texto, tradu√ß√£o autom√°tica, sumariza√ß√£o, etc.\n√â verdade, no entanto, que esses modelos podem n√£o ser capazes de, por si s√≥, lidar com tarefas mais espec√≠ficas. Por exemplo, suponha que queremos responder a perguntas sobre os dados de uma determinada empresa, um conjunto espec√≠fico de artigos ou discursos‚Ä¶ enfim, qualquer coisa do g√™nero. Como esses modelos s√£o generalistas por natureza, eles provavelmente n√£o ser√£o capazes de fornecer respostas espec√≠ficas o suficiente para os nossos objetivos. Al√©m disso, os modelos s√£o treinados com dados de at√© uma determinada data, o que significa que eles podem n√£o ser capazes de fornecer detalhes sobre eventos mais recentes.\nNesse caso, existem duas abordagens principais que podemos seguir: (i) fine-tuning, que consiste essencialmente em ajustar os par√¢metros dessas redes neurais usando um conjunto de dados mais espec√≠fico; e (ii) RAG pipelines, em que combinamos os modelos de linguagem com bases de conhecimento pr√≥prias, oferecendo contextos mais espec√≠ficos para a gera√ß√£o de respostas.\nA abordagem de fine-tuning, apesar de eficaz, pode ser muito custosa, especialmente se levarmos em considera√ß√£o que dependemos de conjuntos de dados grandes o suficiente para fazer alguma diferen√ßa no desempenho do modelo. Por isso, RAG pipelines podem ser uma alternativa prefer√≠vel e mais simples em v√°rios casos.\nTenho trabalhado com alguma frequ√™ncia com RAG pipelines nos √∫ltimos meses e j√° apliquei essa abordagem em projetos de naturezas bastante distintas ‚Äì por exemplo, desde responder perguntas sobre bases de dados em SQL at√© a constru√ß√£o de chatbots para responder perguntas sobre documentos em PDF. Neste post, meu objetivo √© explicar o que s√£o RAG pipelines e como constru√≠-los, utilizando um exemplo pr√°tico com o LlamaIndex e a API da OpenAI."
  },
  {
    "objectID": "blog/rag/index.html#introdu√ß√£o",
    "href": "blog/rag/index.html#introdu√ß√£o",
    "title": "RAG pipelines: o que s√£o e como constru√≠-los",
    "section": "",
    "text": "Hoje em dia temos acesso uma quantidade razoavelmente extensa de Large Language Models dispon√≠veis para uso, como os oferecidos pela OpenAI, Meta, Anthropic e xAI. Essas redes neurais ‚Äúgrandes‚Äù, normalmente caracterizadas dessa maneira em fun√ß√£o do enorme n√∫mero de par√¢metros estimados durante o processo de treinamento, s√£o frequentemente chamados de foundation models. Tratam-se de modelos treinados em quantidades massivas de dados e que, no fim das contas, servem de base para uma s√©rie de aplica√ß√µes de naturezas distintas, como gera√ß√£o de texto, tradu√ß√£o autom√°tica, sumariza√ß√£o, etc.\n√â verdade, no entanto, que esses modelos podem n√£o ser capazes de, por si s√≥, lidar com tarefas mais espec√≠ficas. Por exemplo, suponha que queremos responder a perguntas sobre os dados de uma determinada empresa, um conjunto espec√≠fico de artigos ou discursos‚Ä¶ enfim, qualquer coisa do g√™nero. Como esses modelos s√£o generalistas por natureza, eles provavelmente n√£o ser√£o capazes de fornecer respostas espec√≠ficas o suficiente para os nossos objetivos. Al√©m disso, os modelos s√£o treinados com dados de at√© uma determinada data, o que significa que eles podem n√£o ser capazes de fornecer detalhes sobre eventos mais recentes.\nNesse caso, existem duas abordagens principais que podemos seguir: (i) fine-tuning, que consiste essencialmente em ajustar os par√¢metros dessas redes neurais usando um conjunto de dados mais espec√≠fico; e (ii) RAG pipelines, em que combinamos os modelos de linguagem com bases de conhecimento pr√≥prias, oferecendo contextos mais espec√≠ficos para a gera√ß√£o de respostas.\nA abordagem de fine-tuning, apesar de eficaz, pode ser muito custosa, especialmente se levarmos em considera√ß√£o que dependemos de conjuntos de dados grandes o suficiente para fazer alguma diferen√ßa no desempenho do modelo. Por isso, RAG pipelines podem ser uma alternativa prefer√≠vel e mais simples em v√°rios casos.\nTenho trabalhado com alguma frequ√™ncia com RAG pipelines nos √∫ltimos meses e j√° apliquei essa abordagem em projetos de naturezas bastante distintas ‚Äì por exemplo, desde responder perguntas sobre bases de dados em SQL at√© a constru√ß√£o de chatbots para responder perguntas sobre documentos em PDF. Neste post, meu objetivo √© explicar o que s√£o RAG pipelines e como constru√≠-los, utilizando um exemplo pr√°tico com o LlamaIndex e a API da OpenAI."
  },
  {
    "objectID": "blog/rag/index.html#o-que-s√£o-pipelines-de-rag-e-para-que-servem",
    "href": "blog/rag/index.html#o-que-s√£o-pipelines-de-rag-e-para-que-servem",
    "title": "RAG pipelines: o que s√£o e como constru√≠-los",
    "section": "O que s√£o pipelines de RAG e para que servem?",
    "text": "O que s√£o pipelines de RAG e para que servem?\nRAG significa Retrieval-Augmented Generation. Em termos simplificados, estamos basicamente inserindo um contexto mais espec√≠fico para a gera√ß√£o de respostas de um modelo de linguagem. A expectativa √© que, dado o contexto necess√°rio, o modelo seja capaz de gerar respostas mais adequadas para a tarefa em quest√£o.\nPense na situa√ß√£o em que voc√™ trabalha em uma empresa que possui uma s√©rie de documentos em PDF com diretrizes de atua√ß√£o dos funcion√°rios. A maneira mais trabalhosa de responder perguntas a respeito de cada um deles seria, √© claro, ler cada documento e tirar conclus√µes a partir da leitura.\nPor outro lado, poder√≠amos construir um esquema de conversa√ß√£o de um chatbot com esses documentos para facilitar a recupera√ß√£o de informa√ß√µes. Pense que o funcion√°rio pode ter recebido um e-mail de um endere√ßo suspeito e quer saber, com certa rapidez, qual √© a medida que ele deve tomar em seguida.\n√â claro, o funcion√°rio poderia ler os documentos e procurar a diretriz de atua√ß√£o definida. Ele tamb√©m poderia fazer uma pergunta para o ChatGPT e receber uma resposta razo√°vel com diretrizes gerais de seguran√ßa da informa√ß√£o. Mas, se voc√™ alimenta um modelo modelo com as diretrizes da sua pr√≥pria empresa, a resposta ser√° muito mais efetiva e r√°pida. √â isso que um RAG faz: ele informa ao modelo de linguagem quais partes desses documentos s√£o mais relevantes para as suas perguntas, permitindo que ele gere respostas mais precisas.\nA ideia de embedding √© central para a constru√ß√£o desse tipo de pipeline. Grosso modo, um embedding √© uma representa√ß√£o num√©rica de um texto ‚Äî muito √∫til porque, al√©m de permitir a realiza√ß√£o de c√°lculos matem√°ticos, tamb√©m √© capaz de incorporar a sem√¢ntica (isto √©, o significado) do texto.\nNo caso do RAG, esses embeddings s√£o utilizados para calcular a similaridade entre o texto de entrada ‚Äì ou seja, a sua pergunta ‚Äì e os documentos que servir√£o de contexto para a gera√ß√£o de respostas. Com isso, podemos identificar quais partes desses documentos s√£o mais relevantes para a pergunta e, a partir da√≠, alimentar o modelo de linguagem com essas informa√ß√µes.\nA imagem abaixo √© uma representa√ß√£o visual de um pipeline de RAG:\n\n\n\nRepresenta√ß√£o visual de um pipeline de RAG inspirado neste artigo do Medium.\n\n\nBom, isso √© o RAG! Na realidade, se utilizarmos ferramentas como o LlamaIndex, podemos construir esses pipelines de maneira razoavelmente simples.\nA seguir, vou mostrar uma implementa√ß√£o de um RAG integrando modelos oferecidos pela OpenAI com o LlamaIndex. Embora o exemplo seja bastante simples, a ideia √© que ele possa ser facilmente adaptado para outras tarefas."
  },
  {
    "objectID": "blog/rag/index.html#aplica√ß√£o-com-llamaindex-e-api-da-openai",
    "href": "blog/rag/index.html#aplica√ß√£o-com-llamaindex-e-api-da-openai",
    "title": "RAG pipelines: o que s√£o e como constru√≠-los",
    "section": "Aplica√ß√£o com LlamaIndex e API da OpenAI",
    "text": "Aplica√ß√£o com LlamaIndex e API da OpenAI\nUsando os termos do LlamaIndex1, nossos documentos s√£o ingeridos por containers da classe Document, que s√£o compostos por Nodes ‚Äî cada um deles guardando parte do documento em quest√£o. A √∫ltima etapa √© gerar os embeddings do texto nesses Nodes e, a partir da√≠, podemos calcular a similaridade entre eles e o texto de entrada.\nComo exemplo, vamos testar fazer perguntas sobre a opini√£o de parlamentares brasileiros a respeito da reforma tribut√°ria. Como contexto, vamos utilizar esse documento fornecido pela C√¢mara dos Deputados com 23 discursos de parlamentares sobre o tema ao longo do m√™s de mar√ßo de 2021: [link].\nPrimeiro, vamos perguntar para o ChatGPT (mais especificamente utilizando o modelo gpt-4o) sobre a opini√£o dos parlamentares a respeito da reforma tribut√°ria, sem fornecer nenhum contexto adicional. Depois, vamos fazer a mesma pergunta no contexto de um pipeline RAG, onde o modelo de linguagem ser√° alimentado com o documento que comentei.\nBom, vejamos como isso funciona na pr√°tica. Primeiro, vamos fazer a pergunta para o ChatGPT:\nimport os\nimport openai\n\nos.environ[\"OPENAI_API_KEY\"] = \"sua_chave_de_api\"\nopenai.api_key = os.environ[\"OPENAI_API_KEY\"]\nclient = openai.OpenAI()\n\nresponse = client.chat.completions.create(model=\"gpt-4o-mini\",\n                                          messages=[{\"role\": \"user\", \"content\": \"Qual era a t√¥nica das rela√ß√µes Executivo-Legislativo na Primeira Rep√∫blica? Responda em um par√°grafo curto.\"}],\n                                          temperature=0,\n                                          )\nA resposta que obtemos √© a seguinte:\n\nNa Primeira Rep√∫blica Brasileira (1889-1930), as rela√ß√µes entre o Executivo e o Legislativo foram marcadas por um forte clientelismo e uma pol√≠tica de alian√ßas, onde o presidente buscava apoio no Congresso Nacional para garantir a governabilidade. O sistema pol√≠tico era dominado por oligarquias regionais, especialmente de S√£o Paulo e Minas Gerais, que exerciam grande influ√™ncia sobre as decis√µes legislativas. A pr√°tica do ‚Äúcoronelismo‚Äù e a troca de favores entre os pol√≠ticos eram comuns, resultando em um Executivo que frequentemente se via dependente do Legislativo para a aprova√ß√£o de suas propostas. Essa din√¢mica, por sua vez, gerou instabilidade e crises pol√≠ticas, culminando na Revolu√ß√£o de 1930, que p√¥s fim √† Primeira Rep√∫blica.\n\nConforme o esperado, ela refor√ßa a interpreta√ß√£o cl√°ssica sobre o per√≠odo e sequer menciona vis√µes mais recentes. Agora, vamos testar fazer a mesma pergunta a um pipeline que integra o meu artigo.\nPrimeiro, vamos carregar as bibliotecas:\nimport os\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\nfrom llama_index.embeddings.openai import OpenAIEmbedding\nfrom llama_index.core.retrievers import VectorIndexRetriever\nfrom llama_index.llms.openai import OpenAI\nAgora, vamos definir os modelos que ser√£o utilizados ‚Äî tanto o modelo respons√°vel por dar a resposta quanto o modelo que ser√° utilizado para mapear os textos em embeddings:\n# define o modelo de geracao de texto\nmodel = OpenAI(\n            model=\"gpt-4o-mini\",\n            temperature=0.0,\n            api_key=os.environ[\"OPENAI_API_KEY\"],\n)\n\n# define o modelo de embedding\nSettings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", api_key=os.environ[\"OPENAI_API_KEY\"])\nDepois disso, lemos os documentos (neste caso, apenas um artigo guardado numa pasta documents), criamos o √≠ndice e definimos uma query_engine, respons√°vel por recuperar as partes dos documentos que s√£o mais similares ao texto de entrada:\n# carrega os documentos no diretorio\ndocuments = SimpleDirectoryReader(input_dir='documents').load_data()\n\n# cria o index\nindex = VectorStoreIndex.from_documents(documents)\n\n# cria a query_engine\nquery_engine = index.as_query_engine(\n    similarity_top_k = 1,\n    llm=model,\n)\nNote que, nesse c√≥digo em particular, estou utilizando o similarity_top_k = 1, o que significa que a query engine ir√° recuperar apenas o n√≥ mais similar ao texto de entrada e passar esse n√≥ para o modelo de linguagem, que vai incorporar esse contexto na gera√ß√£o da resposta.\nMinha expectativa √© que, feita a mesma pergunta, o modelo seja capaz de oferecer uma resposta mais espec√≠fica, que leve em considera√ß√£o a literatura mais recente sobre o tema. Vamos ver o que acontece:\n# faz a query ao modelo\nresponse = query_engine.query(\"Qual era a t√¥nica das rela√ß√µes Executivo-Legislativo na Primeira Rep√∫blica? Responda em um par√°grafo curto.\")\nA resposta que obtemos √© a seguinte:\n\nA t√¥nica das rela√ß√µes Executivo-Legislativo na Primeira Rep√∫blica era marcada pela percep√ß√£o de que o Legislativo se tornava uma institui√ß√£o figurativa, com o presidente negociando diretamente com os governadores para evitar a elei√ß√£o de opositores ao Congresso Nacional. No entanto, an√°lises mais recentes indicam que havia conflitos entre os dois poderes, desafiando a vis√£o cl√°ssica sobre a din√¢mica dessas rela√ß√µes.\n\nDe fato, o resultado √© exatamente o esperado. A resposta do modelo incorpora o contexto do artigo, que argumenta contra a vis√£o cl√°ssica sobre a rela√ß√£o entre Executivo e Legislativo na Primeira Rep√∫blica.\nUma caracter√≠stica √∫til desse pipeline √© que conseguimos, al√©m da resposta, identificar quais informa√ß√µes foram inseridas como contexto. Nesse caso em particular, esse pipeline incorporou o resumo do artigo como contexto. Podemos ver isso da seguinte maneira:\nresponse.source_nodes[0].text\nA indexa√ß√£o [0] nesse caso mostra o primeiro item utilizado como contexto adicional. Neste caso apenas um item foi inclu√≠do (dado o similarity_top_k = 1), mas isso pode variar. O resultado √© o seguinte:\n\n\n\nContexto passado para o modelo de linguagem."
  },
  {
    "objectID": "blog/rag/index.html#para-fechar",
    "href": "blog/rag/index.html#para-fechar",
    "title": "RAG pipelines: o que s√£o e como constru√≠-los",
    "section": "Para fechar",
    "text": "Para fechar\nNeste texto, discutimos o conceito e a constru√ß√£o de pipelines do tipo RAG (Retrieval-Augmented Generation), abordando especificamente como essa abordagem permite integrar informa√ß√µes contextuais espec√≠ficas aos modelos fundacionais j√° existentes, como aqueles oferecidos pela OpenAI, Meta e Anthropic.\nAtrav√©s da implementa√ß√£o pr√°tica utilizando o LlamaIndex e um modelo espec√≠fico da OpenAI, mostramos que √© poss√≠vel obter respostas mais precisas e informadas ao integrar documentos como contexto aos modelos. No exemplo apresentado, ficou claro como o uso do pipeline RAG permitiu ao modelo oferecer respostas que n√£o apenas reproduzem interpreta√ß√µes generalistas, mas que incorporam an√°lises mais recentes e espec√≠ficas sobre o tema.\n√â claro, esse √© s√≥ um exemplo de como utilizar um RAG. Na realidade, as possibilidades s√£o vastas e podem ser adaptadas para uma s√©rie de tarefas. Em particular, a integra√ß√£o de RAGs com bases de dados em SQL tem se mostrado bastante promissora, permitindo a constru√ß√£o de sistemas de perguntas e respostas que s√£o capazes de responder a perguntas relativamente complexas sobre conjuntos de dados."
  },
  {
    "objectID": "blog/rag/index.html#footnotes",
    "href": "blog/rag/index.html#footnotes",
    "title": "RAG pipelines: o que s√£o e como constru√≠-los",
    "section": "Notas de rodap√©",
    "text": "Notas de rodap√©\n\n\nEstou me referindo mais especificamente ao VectorStoreIndex, uma t√©cnica de indexa√ß√£o que guarda os n√≥s na forma de uma lista e permite a recupera√ß√£o de apenas alguns deles - os \\(k\\) mais semelhantes ao texto de entrada, sendo \\(k\\) um valor arbitr√°rio definido pelo desenvolvedor da aplica√ß√£o.‚Ü©Ô∏é"
  },
  {
    "objectID": "blog/blog.html",
    "href": "blog/blog.html",
    "title": "üì¨Blog",
    "section": "",
    "text": "RAG pipelines: o que s√£o e como constru√≠-los\n\n\n\nRAG\n\nDeep Learning\n\n\n\nNeste artigo, discuto o conceito de Retrieval-Augmented Generation (RAG). Atrav√©s de um exemplo pr√°tico com o LlamaIndex e a API da OpenAI, demonstro como RAG permite gerar respostas mais precisas, incorporando contexto espec√≠fico a modelos fundacionais j√° existentes.\n\n\n\n\n\nMar 6, 2025\n\n\nFelipe Lamarca\n\n\n\n\n\n\n\n\n\n\n\n\nScience and Statistics: a brief review\n\n\n\nStatistics\n\nScience\n\n\n\nAll models are wrong; some are useful. But why is that? In this article, I review Box‚Äôs (1976) paper on the iterative process of scientific knowledge production.\n\n\n\n\n\nSep 19, 2024\n\n\nFelipe Lamarca\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/box_1976/index.html",
    "href": "blog/box_1976/index.html",
    "title": "Science and Statistics: a brief review",
    "section": "",
    "text": "In practically all disciplines across different fields of Science ‚Äî Natural, Exact, or Social ‚Äî one of the most fundamental discussions debated by scholars is the conceptualization of the discipline in question. In other words, it is a process of identification in order to apply. To conduct research in Statistics, one must know what Statistics is; to produce a historiographical work, one must know what History is, and so on. The task of defining a discipline, however, is not trivial, and more importantly: given a definition, it is certainly not unique, uncontested, or static. This is precisely why, over time, new methods of producing scientific knowledge are suggested and executed.\nThe article Science and Statistics (Box 1976) may, at first glance, seem like an uninteresting digression on how Ronald Fisher (1890-1962) contributed to the evolution of Statistics, in particular, and Science in general, through the methods he developed and improved. On the other hand, a more attentive reading reveals that Box (1976) actually presents his ideal view of how scientific knowledge should be produced1: ‚Äú[‚Ä¶] not by mere theoretical speculation on one hand, nor by the undirected accumulation of practical facts on the other, but rather a motivated iteration between theory and practice [‚Ä¶]‚Äù (Box 1976, 791).\nScientific practice, therefore, is understood by Box (1976) as a kind of loop, or a tentative theory. The researcher analyzes the available theory, makes deductions from it, and compares them to the facts ‚Äî or data ‚Äî that can be accessed. These two pieces of information, of distinct natures, do not necessarily converge, so the theory needs to be adjusted to explain a certain phenomenon. This new theory is compared to the facts again and so on, in a literally iterative process. The question is, therefore, the following: the confrontation between facts and theory produces errors, which imply the need to reassess one or the other element.\nThat is why ‚Äú[‚Ä¶] all models are wrong [‚Ä¶].‚Äù (Box 1976, 792). Naturally, all models are wrong because scientific practice results from a continuous comparison between theory and practice, so no matter how much a scientist elaborates their model in advance, it will likely need to be adapted when confronted with the factual. Box (1976), in this regard, refers to the fact that, in practice, there are no normal distributions or linearity in nature. That is, even though they are incorrect models (because they do not precisely correspond to what is found in nature), they are still useful for making approximations of what is found in the real world, as the result comes from the iteration between theory and practice. From a statistician‚Äôs point of view, the iterative process develops through a stage in which the scientist chooses the best statistical procedures to analyze the data and supports the model; in the next stage, after the analysis, they assume that the model contains errors and apply a series of residual analyses to improve it, and so on.\nThe examples based on a brief biography of Fisher serve, in practice, to illustrate the ‚Äúbest practices‚Äù of acquiring scientific knowledge. They focus on Fisher‚Äôs initial concern with solving some practical problem of scientific relevance and highlight Box (1976)‚Äôs ideal regarding scientific practice: one must not lose sight of the real problem to which certain statistical knowledge is being applied or even developed. Hence, the critiques of Mathematistry2, which is often disconnected from practical issues. It thus reinforces the importance of statisticians who can combine and confront theory and practice to, in fact, produce scientific knowledge."
  },
  {
    "objectID": "blog/box_1976/index.html#footnotes",
    "href": "blog/box_1976/index.html#footnotes",
    "title": "Science and Statistics: a brief review",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn his argument, the author addresses Science in general. Ultimately, we must keep in mind that this proposition does not necessarily encompass all scientific disciplines. Nevertheless, we know that his concern is with the Exact Sciences in general, especially Statistics.‚Ü©Ô∏é\n‚ÄúMathematistry is characterized by development of theory for theory‚Äôs sake which since it seldom touches down with practice, has a tendency to redefine the problem rather than solve it.‚Äù (Box 1976, 798).‚Ü©Ô∏é"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Felipe Lamarca",
    "section": "",
    "text": "Welcome!üëãüèº\nI‚Äôm a data scientist with a degree from the School of Applied Mathematics (FGV EMAp) and a social scientist with a degree from the School of Social Sciences (FGV CPDOC). Currently, I‚Äôm pursuing a Master‚Äôs degree in Political Science at the Institute of Social and Political Studies (IESP-UERJ).\n\nI‚Äôve been working as a data scientist and researcher for a few years now, gaining experience through a variety of projects along the way.\nMy interests include Statistical Modeling, Machine Learning, Deep Learning, and Computational Social Sciences, as well as AI safety and governance. I also have experience in ETL pipelines (SQL and NoSQL), GenAI applications, and survey methods.\nüìß felipe.lamarca@hotmail.com"
  },
  {
    "objectID": "my-work/my-work.html",
    "href": "my-work/my-work.html",
    "title": "üë®üèª‚Äçüíª My work",
    "section": "",
    "text": "My expertise spans the entire data pipeline‚Äîdesigning ETL systems, conducting causal inference, and implementing machine learning models (including Generative AI solutions). Current services include:\n\nEnd-to-end data science and AI development, including RAG pipelines and chatbot solutions\nResearch support (survey design, causal analysis, data visualization)\nTailored training and workshops in data science and programming fundamentals\n\nSelected consulting projects:\nüìå Artplan: Social media data analysis with Deep Learning for user segmentation and image clustering\nüìå Funda√ß√£o Roberto Marinho: AWS data lake implementation (S3, Glue, Lambda), dashboard development (Power BI, Streamlit), and training in Python and ML\nüìå CEBRAP: Technical analysis using OCR, LLMs, and quantitative research support for Prof.¬†Dr.¬†Jos√© Szwako (IESP-UERJ)\nüìå Plataforma CIP√ì: Instructor for data analysis and machine learning training program\nüìå GENI/UFF: Built customized tools using LLMs and RAG pipelines for extracting and interacting with data\nIf you think I can help you in any way, please reach out at felipe.lamarca@hotmail.com.\n\n\n\nI conduct quantitative research (such as opinion polls and other surveys), focusing on data collection, analysis, and visualization. I have participated in projects like the Municipal Management Assessment Survey and Election Polls, while collaborating with Alian√ßa CentroRio to transform citywide conservation and security data into actionable intelligence."
  },
  {
    "objectID": "my-work/my-work.html#current-roles",
    "href": "my-work/my-work.html#current-roles",
    "title": "üë®üèª‚Äçüíª My work",
    "section": "",
    "text": "My expertise spans the entire data pipeline‚Äîdesigning ETL systems, conducting causal inference, and implementing machine learning models (including Generative AI solutions). Current services include:\n\nEnd-to-end data science and AI development, including RAG pipelines and chatbot solutions\nResearch support (survey design, causal analysis, data visualization)\nTailored training and workshops in data science and programming fundamentals\n\nSelected consulting projects:\nüìå Artplan: Social media data analysis with Deep Learning for user segmentation and image clustering\nüìå Funda√ß√£o Roberto Marinho: AWS data lake implementation (S3, Glue, Lambda), dashboard development (Power BI, Streamlit), and training in Python and ML\nüìå CEBRAP: Technical analysis using OCR, LLMs, and quantitative research support for Prof.¬†Dr.¬†Jos√© Szwako (IESP-UERJ)\nüìå Plataforma CIP√ì: Instructor for data analysis and machine learning training program\nüìå GENI/UFF: Built customized tools using LLMs and RAG pipelines for extracting and interacting with data\nIf you think I can help you in any way, please reach out at felipe.lamarca@hotmail.com.\n\n\n\nI conduct quantitative research (such as opinion polls and other surveys), focusing on data collection, analysis, and visualization. I have participated in projects like the Municipal Management Assessment Survey and Election Polls, while collaborating with Alian√ßa CentroRio to transform citywide conservation and security data into actionable intelligence."
  },
  {
    "objectID": "my-work/my-work.html#past-work",
    "href": "my-work/my-work.html#past-work",
    "title": "üë®üèª‚Äçüíª My work",
    "section": "üìÅ Past Work",
    "text": "üìÅ Past Work\n\nüìç Research Assistant @ FGV EBAPE (02/2025 ‚Äì 04/2025)\nI worked on a project to systematize databases for election results from the 1970s, using OCR and large language models to digitize and format historical documents. I worked under the supervision of Prof.¬†Cesar Zucco, Ph.D.\n\n\n\nüìç Data Science Intern @ Strategic Planning Superintendence, FGV (10/2024 ‚Äì 12/2024)\nI integrated advanced Generative AI techniques into institutional data pipelines, identifying opportunities for automation and process improvement. I also prepared presentations for C-level executives, including the President of FGV.\n\n\nüìç Part-time Data Scientist @ Visagio (01/2024 ‚Äì 06/2024)\nI worked on optimization, pricing, A/B testing, and data pipeline automation with SQL. I developed Power BI dashboards and presented KPI-driven insights to senior management.\n\n\nüìç Student Consultant @ Brazilian Sailing Confederation (08/2023 ‚Äì 12/2023)\nI collaborated with colleagues to develop a ranking model for Brazilian sailors, integrating web scraping and machine learning to analyze regatta performance.\n\n\nüìç Junior Research Fellow @ FGV CPDOC (06/2019 ‚Äì 09/2022)\nSupported by an FGV/CNPq scholarship under the supervision of Prof.¬†Jaqueline Porto Zulini, Ph.D., I examined Executive-Legislative relations in the early 20th century, applying both qualitative methods and introductory quantitative text analysis."
  },
  {
    "objectID": "my-work/my-work.html#teaching-assistance",
    "href": "my-work/my-work.html#teaching-assistance",
    "title": "üë®üèª‚Äçüíª My work",
    "section": "üìö Teaching Assistance",
    "text": "üìö Teaching Assistance\n\n\n\nSemester\nCourse\nInstitution\nProfessor\n\n\n\n\n2024.2\nQuantitative Methods II\nFGV CPDOC\nJairo Nicolau, Ph.D.\n\n\n2020.2\nIntroduction to R\nFGV CPDOC\nJimmy Medeiros, Ph.D.\n\n\n\n(Previously, I also worked on ‚ÄúDigital Literacy‚Äù projects and translated materials for the Programming Historian in Portuguese in partnership with Universidade Nova de Lisboa.)"
  },
  {
    "objectID": "my-work/my-work.html#references",
    "href": "my-work/my-work.html#references",
    "title": "üë®üèª‚Äçüíª My work",
    "section": "üìû References",
    "text": "üìû References\n\n\n\n\n\n\n\n\nName\nInstitution\nEmail\n\n\n\n\nProf.¬†Jaqueline Zulini, PhD\nSchool of Social Sciences (FGV CPDOC)\njaqueline dot zulini at fgv dot br\n\n\nProf.¬†Luiz Max Carvalho, PhD\nSchool of Applied Mathematics (FGV EMAp)\nluiz dot fagundes at fgv dot br\n\n\nProf.¬†Renato Rocha Souza, PhD\nUniversity of Vienna\nrenato dot rocha dot souza at univie dot ac dot at\n\n\nProf.¬†Danielle Sanches, PhD\nSchool of Communication, Media and Information (FGV ECMI)\ndanielle dot sanches at fgv dot br\n\n\nProf.¬†Jimmy Medeiros, PhD\nSchool of Social Sciences (FGV CPDOC)\njimmy dot medeiros at fgv dot br"
  },
  {
    "objectID": "projects/data-visualization/index.html",
    "href": "projects/data-visualization/index.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Sketch of Visualizations\nVisualization Project\nExploratory Data Analysis"
  },
  {
    "objectID": "projects/data-visualization/index.html#assignments",
    "href": "projects/data-visualization/index.html#assignments",
    "title": "Data Visualization",
    "section": "",
    "text": "Sketch of Visualizations\nVisualization Project\nExploratory Data Analysis"
  },
  {
    "objectID": "projects/data-visualization/index.html#projects",
    "href": "projects/data-visualization/index.html#projects",
    "title": "Data Visualization",
    "section": "Projects",
    "text": "Projects\n\nProject 1: Voc√™ em Dados\nProject co-developed with Ana Carolina Erthal and Guilherme de Melo.\nSee the project here.\n\n\nProject 2: Visual F1\nProject co-developed with Ana Carolina Erthal and Guilherme de Melo.\nSee the project here."
  },
  {
    "objectID": "projects/deep-learning/index.html",
    "href": "projects/deep-learning/index.html",
    "title": "Deep Learning",
    "section": "",
    "text": "Assignment 1: Transfer Learning (10/10)\nAssignment 2: Semantic Segmentation (10/10)\nAssignment 3: Action Recognition (10/10)\nAssignment 4: Generative Adversarial Networks + Autoencoders (10/10)\nAssignment 5: Deep \\(k\\)-Means (10/10)\nPresentation: YUN, S. et al.¬†Graph Transformer Networks. In: NeurIPS, 2019. [pdf] [slides]\nAll notebooks were ran on Google Colab.\nThe repository also contains the cheatsheets folder with some useful concepts of Deep Learning I have written down during the course.\n\n\n\nCitationBibTeX citation:@online{lamarca2023,\n  author = {Lamarca, Felipe and Carolina Erthal, Ana},\n  title = {Deep {Learning}},\n  date = {2023-12-05},\n  url = {https://github.com/felipelmc/Deep-Learning},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLamarca, Felipe, and Ana Carolina Erthal. 2023. ‚ÄúDeep\nLearning.‚Äù December 5, 2023. https://github.com/felipelmc/Deep-Learning."
  },
  {
    "objectID": "projects/projects.html",
    "href": "projects/projects.html",
    "title": "‚öôProjects",
    "section": "",
    "text": "SciELO-Summarizer\n\n\n\nComputational Social Sciences\n\nDeep Learning\n\n\n\nSciELO-Summarizer consists of a summarizer for scientific articles in Portuguese. It scrapes the content of the article from the SciELO website and generates a personalized summary for the user using Large Language Models (LLMs), especifically Llama3.\n\n\n\n\n\nJul 15, 2024\n\n\nFelipe Lamarca, Claudia Fernandes, Daniel Zacarias, Gabriellen Carmo, Lauriano Benazzi, Marcelle Amaral, Talita Ribeiro\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning\n\n\n\nDeep Learning\n\n\n\nAssignments and presentation developed in the scope of the Deep Learning discipline, taught by Professor D√°rio Oliveira (FGV EMAp). Co-authored with @anacarolerthal, whom I thank for the ongoing partnership.\n\n\n\n\n\nDec 5, 2023\n\n\nFelipe Lamarca, Ana Carolina Erthal\n\n\n\n\n\n\n\n\n\n\n\n\nDataGrid\n\n\n\nAlgorithms\n\n\n\nImplementation of search, sorting & selection algorithms to build an efficient datagrid. Project developed within the scope of the Algorithm Design and Analysis discipline, lectured by Professor Thiago Pinheiro de Ara√∫jo (FGV EMAp).\n\n\n\n\n\nOct 18, 2023\n\n\nFelipe Lamarca, Cristiano Larr√©a\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Modeling\n\n\n\nStatistics\n\nComputational Social Sciences\n\n\n\nThis repository gathers the data, scripts, and analyses conducted for the final project of the Statistical Modeling course, taught by Professor Luiz Max Fagundes de Carvalho (FGV EMAp) (@maxbiostat). The objective was to apply modeling, inference, and prediction techniques, learned throughout this course and the Statistical Inference course, to real-world data.\n\n\n\n\n\nJul 15, 2023\n\n\nFelipe Lamarca\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\nMachine Learning\n\nStatistics\n\n\n\nThis repository contains all the assignments and the project I did for the Machine Learning course at the School of Applied Mathematics of the Getulio Vargas Foundation (FGV EMAp). The course was taught by Professor Diego Mesquita.\n\n\n\n\n\nJul 5, 2023\n\n\nFelipe Lamarca\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\nData Visualization\n\n\n\nRepository where I keep all the assignments and projects developed in the scope of the Data Visualization discipline, taught by Professor Jorge Poco (@jpocom) (FGV EMAp).\n\n\n\n\n\nJun 30, 2023\n\n\nFelipe Lamarca, Ana Carolina Erthal, Guilherme de Melo\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/statistical-modeling/index.html",
    "href": "projects/statistical-modeling/index.html",
    "title": "Statistical Modeling",
    "section": "",
    "text": "I chose to analyze the electoral dynamics for the position of federal deputy in the 2022 elections. Specifically, I explore multilevel (hierarchical) models, logistic regression, and model evaluation methods, such as AUC, AIC, accuracy, and \\(R^2\\). Additionally, I engage with part of the Political Science literature that uses Statistical Modeling techniques to extract information about Brazilian elections.\nThis work resulted in this report, the summary of which is as follows:"
  },
  {
    "objectID": "projects/statistical-modeling/index.html#about-the-project",
    "href": "projects/statistical-modeling/index.html#about-the-project",
    "title": "Statistical Modeling",
    "section": "",
    "text": "I chose to analyze the electoral dynamics for the position of federal deputy in the 2022 elections. Specifically, I explore multilevel (hierarchical) models, logistic regression, and model evaluation methods, such as AUC, AIC, accuracy, and \\(R^2\\). Additionally, I engage with part of the Political Science literature that uses Statistical Modeling techniques to extract information about Brazilian elections.\nThis work resulted in this report, the summary of which is as follows:"
  },
  {
    "objectID": "projects/statistical-modeling/index.html#who-wants-to-be-a-deputy-a-multilevel-analysis-of-the-2022-elections-for-the-chamber-of-deputies",
    "href": "projects/statistical-modeling/index.html#who-wants-to-be-a-deputy-a-multilevel-analysis-of-the-2022-elections-for-the-chamber-of-deputies",
    "title": "Statistical Modeling",
    "section": "Who Wants to Be a Deputy? A Multilevel Analysis of the 2022 Elections for the Chamber of Deputies",
    "text": "Who Wants to Be a Deputy? A Multilevel Analysis of the 2022 Elections for the Chamber of Deputies\nWhat increases a candidate‚Äôs chances of being elected? The specialized Political Science literature has sought to answer this question through various approaches over time. This work provides a statistical analysis of the 2022 electoral data to evaluate what impacts the chances of federal deputy candidates being elected in Brazil. Statistical modeling techniques are applied, including multilevel logistic regression models and metrics for evaluating the explanatory and predictive capacity of models. The results suggest that campaign expenditures, with variations between parties, account for a significant portion of the results at the polls.\nKeywords: Legislative Elections; Logistic Regression; Multilevel Models; Model Evaluation.\nThe work has been completed and received the highest grade."
  }
]